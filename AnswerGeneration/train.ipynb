{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5doOG9iIsYg",
        "colab_type": "text"
      },
      "source": [
        "#**Answer Generation 任務**\n",
        "資料集引用台達閱讀理解資料集\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXKj-j9ZfzFu",
        "colab_type": "text"
      },
      "source": [
        "### pytorch環境設定(https://pytorch.org)\n",
        "選擇適合的版本並複製指令貼到本專案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mUPOCc9f1fT",
        "colab_type": "code",
        "outputId": "7195b9f9-42f4-45ac-eb22-14d0f09fd6a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CQquGs0f2-E",
        "colab_type": "code",
        "outputId": "b19362fd-c4e7-48b8-9b48-b8901380f18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#確定有安裝成功\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTf31QYifint",
        "colab_type": "text"
      },
      "source": [
        "### **取得 google drive 存取權限**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CqfyK8SIo2o",
        "colab_type": "code",
        "outputId": "8c8e6af4-8dda-40f5-f2b4-94c43574775c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Pg2ymcGyg2",
        "colab_type": "code",
        "outputId": "f49ea8de-79c1-49ab-d292-9c9f29fa03dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/Bert/AnswerGeneration/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Bert/AnswerGeneration\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdtNtU4DG02O",
        "colab_type": "code",
        "outputId": "7d2d4a3a-00e7-4fd0-c4f7-30af72bb9e3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "albert\tpredict.ipynb  test.json  train.ipynb  train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RZSbmA5iyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33rIRZCIj2yW",
        "colab_type": "text"
      },
      "source": [
        "### **下載dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zDOYOkyj67s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def fileExists(filepath):\n",
        "  return  os.path.exists(filepath)\n",
        "#我把資料集放到我的github\n",
        "if fileExists('train.json')!=True:\n",
        "  !wget -O train.json https://raw.githubusercontent.com/harry83528/NLP-BERT-Experience/master/AnswerGeneration/DataSet/DRCD_training.json\n",
        "if fileExists('test.json')!=True:\n",
        "  !wget -O test.json https://raw.githubusercontent.com/harry83528/NLP-BERT-Experience/master/AnswerGeneration/DataSet/DRCD_test.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps1T6kk6j9s9",
        "colab_type": "code",
        "outputId": "e0f6b114-cfb9-40da-fb4c-97903cdd40ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "albert\tpredict.ipynb  test.json  train.ipynb  train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5GCPnyJW94G",
        "colab_type": "text"
      },
      "source": [
        "##解析json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v4klQQiZwaL",
        "colab_type": "code",
        "outputId": "f61b63db-f4e5-45fc-c9d9-648103b0b83f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install ijson"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ijson in /usr/local/lib/python3.6/dist-packages (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqaPMTwBXeLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ijson\n",
        "# 讀取數據\n",
        "def LoadData(filepath):\n",
        "    context = []\n",
        "    question = []\n",
        "    answersText = []\n",
        "    count=0\n",
        "    contextId=''\n",
        "    with open(filepath) as file:\n",
        "        parser = ijson.parse(file)\n",
        "        contextId=''\n",
        "        context2=''\n",
        "        question2=''\n",
        "        for prefix, event, value in parser:\n",
        "            # print('prefix',prefix)\n",
        "            # print('event',event)\n",
        "            # print('value',value)\n",
        "            if prefix==\"data.item.paragraphs.item.id\":\n",
        "              #print('contextId',value)          \n",
        "              contextId=value\n",
        "            if prefix==\"data.item.paragraphs.item.context\":\n",
        "              #print('context',value)\n",
        "              context2=value\n",
        "            if prefix==\"data.item.paragraphs.item.qas.item.question\":\n",
        "              question2=value\n",
        "              #print('question',value)\n",
        "              #print('contextId',contextId)              \n",
        "            if prefix==\"data.item.paragraphs.item.qas.item.answers.item.text\":\n",
        "              #print('context2',context2)\n",
        "              #print('answersText',value)\n",
        "              #print('contextId',contextId)\n",
        "              context.append(context2)\n",
        "              question.append(question2)\n",
        "              answersText.append(value+\"[SEP]\")\n",
        "            #print(\"---\"*50)\n",
        "            # count=count+1\n",
        "            # if count==51:\n",
        "            #   break;\n",
        "    return context,question,answersText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQKy9Cntu0AA",
        "colab_type": "text"
      },
      "source": [
        "##下載Albert並放在albert目錄"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4dSWkZOu2rA",
        "colab_type": "code",
        "outputId": "67998217-7f75-4782-b1c5-2b4bcdef7bec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/harry83528/albert-zh-for-pytorch-transformers.git albert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'albert' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMHSR-23g14r",
        "colab_type": "text"
      },
      "source": [
        "### **安裝所需的函式庫-HuggingFace 團隊將 GitHub 專案大翻新並更名成 transformers(Install the Hugging Face Library)**\n",
        "https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noekyOhAg3qt",
        "colab_type": "code",
        "outputId": "c48eecd3-e164-4c2d-c3c1-ebb2359d3f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Using cached https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 3.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3022928110c4dce9db582099032d0cff0b43be33dd2e27f9459d3e3b105cc813\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKjsatBug5tU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmTaWgVBvGx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def use_model(model_name, config_file_path, model_file_path, vocab_file_path, num_labels):\n",
        "    # 選擇模型並加載設定\n",
        "    if(model_name == 'bert'):\n",
        "        from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
        "        model_config, model_class, model_tokenizer = (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
        "        config = model_config.from_pretrained(config_file_path,num_labels = num_labels)\n",
        "        model = model_class.from_pretrained(model_file_path, from_tf=bool('.ckpt' in 'bert-base-chinese'), config=config)\n",
        "        tokenizer = model_tokenizer(vocab_file=vocab_file_path)\n",
        "        return model, tokenizer\n",
        "    elif(model_name == 'albert'):\n",
        "        from albert.albert_zh import AlbertConfig, AlbertTokenizer, AlbertForMaskedLM\n",
        "        model_config, model_class, model_tokenizer = (AlbertConfig, AlbertForMaskedLM, AlbertTokenizer)\n",
        "        config = model_config.from_pretrained(config_file_path)\n",
        "        #config = model_config.from_pretrained(config_file_path,num_labels = num_labels) # AlbertForMaskedLM 不需分類,不用設num_labels\n",
        "        model = model_class.from_pretrained(model_file_path, config=config)\n",
        "        tokenizer = model_tokenizer.from_pretrained(vocab_file_path)\n",
        "        return model, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIfZwFodk2K_",
        "colab_type": "text"
      },
      "source": [
        "### **指定想要用的模型是哪一種**\n",
        "到以下網址查詢模型與函數資訊\n",
        "https://huggingface.co/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKfJFywXXTeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#通常我會在這設一些通用模型參數\n",
        "# PRETRAINED_MODEL_Name =\"bert-base-chinese\"  # 指定 想要用的預訓練模型\n",
        "trained_Model_Path = \"trained_model\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRc990adg8W-",
        "colab_type": "text"
      },
      "source": [
        "## **引入需要⽤到的函式庫**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFqvPMRjg7nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from transformers import BertConfig, BertTokenizer, BertForMaskedLM, AdamW # 此任務不用\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F # 激勵函數\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFROMAiKl8ui",
        "colab_type": "text"
      },
      "source": [
        "### **載入資料並將資料轉換成輸入格式(Token Embeddings、Segment Embeddings、Position Embeddings)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TyhZ9rNXgEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_feature(tokenizer, context, question, answer, token_embeddings, segement_embeddings, attention_mask, masked_labels, max_seq_len):\n",
        "    segement_id = []\n",
        "    attention_id = []\n",
        "    masked_id = []\n",
        "\n",
        "    context_string = \"[CLS]\" + context + \"[SEP]\"\n",
        "    context_tokenize = tokenizer.tokenize(context_string)\n",
        "    for i in range(len(context_tokenize)):\n",
        "        segement_id.append(0) # 第0組\n",
        "        attention_id.append(1) # attention都是1告訴bert是有意義的文字\n",
        "        masked_id.append(-1)  # 不是被mask的字都是-1\n",
        "\n",
        "    question_string = question + \"[SEP]\"\n",
        "    question_tokenize = tokenizer.tokenize(question_string)\n",
        "    for i in range(len(question_tokenize)):\n",
        "        segement_id.append(1) # 第1組\n",
        "        attention_id.append(1) # attention都是1告訴bert是有意義的文字\n",
        "        masked_id.append(-1)  # 不是被mask的字都是-1\n",
        "\n",
        "    answer_tokenize = tokenizer.tokenize(answer)\n",
        "    #print('answer_tokenize:',answer_tokenize); #['西', '元', '1994', '年', '5', '月', '28', '日','<SEP>']\n",
        "    for index, answer in enumerate(answer_tokenize):\n",
        "        input_string = context_tokenize + question_tokenize\n",
        "        token_id = []\n",
        "        input_segement_id = segement_id + []\n",
        "        input_attention_id = attention_id + []\n",
        "        input_masked_id = masked_id + []\n",
        "        \n",
        "        for i in range(index):\n",
        "            input_string.append(answer_tokenize[i])\n",
        "            input_segement_id.append(2) # 第2組\n",
        "            input_attention_id.append(1) # attention都是1告訴bert是有意義的文字\n",
        "            input_masked_id.append(-1)  # 不是被mask的字都是-1\n",
        "\n",
        "        input_string.append('[MASK]')\n",
        "        input_segement_id.append(2) # 第2組\n",
        "        input_attention_id.append(1) # attention都是1告訴bert是有意義的文字\n",
        "        input_masked_id.append(tokenizer.convert_tokens_to_ids(answer))  # 被mask的掉字給該單詞tokens id\n",
        "\n",
        "        token_id = tokenizer.convert_tokens_to_ids(input_string)\n",
        "        # print('input_string',input_string)\n",
        "        # print('token_id',token_id)\n",
        "        # print('input_segement_id',input_segement_id)\n",
        "        # print('input_attention_id',input_attention_id)\n",
        "        # print('input_masked_id',input_masked_id)\n",
        "        # print('---'*20)\n",
        "\n",
        "        token_embeddings.append(token_id)\n",
        "        segement_embeddings.append(input_segement_id)\n",
        "        attention_mask.append(input_attention_id)\n",
        "        masked_labels.append(input_masked_id)\n",
        "\n",
        "        if len(token_id) > max_seq_len:\n",
        "            max_seq_len = len(token_id)\n",
        "\n",
        "    return max_seq_len\n",
        "def convert_data_to_feature(filepath,tokenizer):\n",
        "\n",
        "    context,question,answersText = LoadData(filepath)\n",
        "\n",
        "    print(len(context),len(question),len(answersText))\n",
        "    print(context[1000])\n",
        "    print(question[1000])\n",
        "    print(answersText[1000])\n",
        "\n",
        "    token_embeddings = []\n",
        "    segement_embeddings = []\n",
        "    attention_mask = []\n",
        "    masked_labels = []\n",
        "\n",
        "    max_seq_len = 0\n",
        "    conform_count = 0\n",
        "    for i in range(0,len(context)-1):\n",
        "        context_word_piece_list=tokenizer.tokenize(context[i])\n",
        "        question_word_piece_list=tokenizer.tokenize(question[i])\n",
        "        answersText_word_piece_list=tokenizer.tokenize(answersText[i])\n",
        "        if len(context_word_piece_list) <= 450 and len(question_word_piece_list) <= 42 and len(answersText_word_piece_list) <= 16:\n",
        "            max_seq_len = to_feature(tokenizer, context[i], question[i], answersText[i],token_embeddings,segement_embeddings,attention_mask,masked_labels,max_seq_len)\n",
        "            conform_count =conform_count+1\n",
        "\n",
        "    print(\"最大長度:\",max_seq_len)\n",
        "    print(\"有符合限制長度的總數\",conform_count)\n",
        "\n",
        "    assert max_seq_len <= 512 # 小於BERT-base長度限制\n",
        "    max_seq_len = 512\n",
        "\n",
        "    # 不足長度都補齊長度,都補0\n",
        "    for item in token_embeddings:\n",
        "        while len(item)<max_seq_len:\n",
        "            item.append(0)\n",
        "\n",
        "    for item in segement_embeddings:\n",
        "        while len(item)<max_seq_len:\n",
        "            item.append(0)\n",
        "\n",
        "    for item in attention_mask:\n",
        "        while len(item)<max_seq_len:\n",
        "            item.append(0)\n",
        "\n",
        "    for item in masked_labels:\n",
        "        while len(item)<max_seq_len:\n",
        "            item.append(-1)\n",
        "    \n",
        "    assert len(token_embeddings) == len(segement_embeddings) and len(token_embeddings) == len(attention_mask) and len(token_embeddings) == len(masked_labels)\n",
        "\n",
        "    #為了使⽤⽅便，把三個input embedding及 label 包成 object======================\n",
        "    data_features = {'token_embeddings':token_embeddings,\n",
        "                    'segement_embeddings':segement_embeddings,\n",
        "                    'attention_mask':attention_mask,\n",
        "                    'masked_labels':masked_labels}\n",
        "\n",
        "    return data_features\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJa9hDtHha7d",
        "colab_type": "text"
      },
      "source": [
        "### **輸入格式轉Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmfCA--5giIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeDataset(data_feature):\n",
        "    token_embeddings = data_feature['token_embeddings']\n",
        "    segement_embeddings = data_feature['segement_embeddings']\n",
        "    attention_mask = data_feature['attention_mask']\n",
        "    masked_labels = data_feature['masked_labels']\n",
        "\n",
        "    all_token_embeddings = torch.tensor([token_id for token_id in token_embeddings], dtype=torch.long)\n",
        "    all_segement_embeddings = torch.tensor([segement_id for segement_id in segement_embeddings], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([attention_id for attention_id in attention_mask], dtype=torch.long)\n",
        "    all_masked_labels = torch.tensor([masked_lm_id for masked_lm_id in masked_labels], dtype=torch.long)\n",
        "\n",
        "    return TensorDataset(all_token_embeddings, all_segement_embeddings, all_attention_mask, all_masked_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD-OABDMhe8E",
        "colab_type": "text"
      },
      "source": [
        "## **確定 Embedding 是否正確**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJvLZ1tH2rgf",
        "colab_type": "text"
      },
      "source": [
        "## **分割數據**\n",
        "把資料切成訓練集跟測試集\n",
        "對於 Supervised Learning 來說，我們使⽤標註資料（Labeled Data）來做訓練\n",
        "因此我們需要訓練資料（Training Data）\n",
        "但是我們蒐集到的資料卻不能全部拿來做訓練\n",
        "因為我們必須要保留⼀些當作測試資料（Testing Data）來評估模型表現\n",
        "這些資料必須跟訓練資料是完全不同的，否則就有作弊的嫌疑\n",
        "所以我們會把我們的資料切成訓練集與測試集，通常會保留比較多當作訓練資料\n",
        "（60%~80%），其餘做測試資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST5ZHdBt2BA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(full_dataset, split_rate=0.8):  \n",
        "    train_size = int(split_rate * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    return train_dataset,test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXGgr6iUhiRd",
        "colab_type": "text"
      },
      "source": [
        "### **Fine-Tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN6qIwfDhlVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 計算正確值 (此次任務為生成不需要)\n",
        "# def compute_accuracy(y_pred, y_target):\n",
        "#     _, y_pred_indices = y_pred.max(dim=1)\n",
        "#     n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "#     return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTo7tDYqT7a8",
        "colab_type": "code",
        "outputId": "4f16cd78-8e2b-4eaa-9caf-7c0dafcfa895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# RuntimeError: Error(s) in loading state_dict for AlbertForMaskedLM:\n",
        "# \tsize mismatch for bert.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 312]) from checkpoint, the shape in current model is torch.Size([3, 312])\n",
        "# if len(error_msgs) > 0:\n",
        "#     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "#                         model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "#設定模型參數 Start =============================================================\n",
        "#Step1:初始化加載模型 Start======================================================\n",
        "model_setting = {\n",
        "    \"model_name\":\"albert\", \n",
        "    \"config_file_path\":\"albert/albert_tiny/config.json\", \n",
        "    \"model_file_path\":\"albert/albert_tiny/pytorch_model.bin\", \n",
        "    \"vocab_file_path\":\"albert/albert_tiny/vocab.txt\",\n",
        "    \"num_labels\": -1 # 分幾類(AlbertForMaskedLM 不需分類,不用設num_labels,在這隨便亂設)\n",
        "}\n",
        "#初始化加載模型\n",
        "model, tokenizer = use_model(**model_setting)\n",
        "# config = BertConfig.from_pretrained(PRETRAINED_MODEL_Name)\n",
        "# model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_Name, from_tf=bool('.ckpt' in PRETRAINED_MODEL_Name), config=config)\n",
        "#Step1:初始化加載模型 END========================================================\n",
        "\n",
        "#Step2:指定硬體裝置 Start========================================================\n",
        "# setting device\n",
        "#你電腦的 GPU 能否被 PyTorch 調用,如果不行就使用CPU \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"using device\",device)\n",
        "model.to(device)\n",
        "#Step2:指定硬體裝置 END==========================================================\n",
        "\n",
        "#Step3:將訓練資料讀入並且組建BERT輸入格式 Start===================================\n",
        "train_data_feature = convert_data_to_feature('train.json',tokenizer)\n",
        "test_data_feature = convert_data_to_feature('test.json',tokenizer)\n",
        " #Step3:將訓練資料讀入並且組建BERT輸入格式 END====================================\n",
        "\n",
        "#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader Srart======\n",
        "# full_dataset = makeDataset(train_data_feature)\n",
        "# train_dataset, test_dataset = split_dataset(full_dataset, 0.8)\n",
        "train_dataset = makeDataset(train_data_feature)\n",
        "test_dataset = makeDataset(test_data_feature)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset ,batch_size=2 ,shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset ,batch_size=1 ,shuffle=True)\n",
        "#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader END========\n",
        "\n",
        "#準備優化器 Start===============================================================\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "Learning_rate = 5e-6       # 學習率\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=Learning_rate, eps=1e-8)\n",
        "#準備優化器 End=================================================================\n",
        "\n",
        "\n",
        "#設定模型參數 End==============================================================="
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device cuda\n",
            "26936 26936 26936\n",
            "滿洲國境內鐵路線稠密，鐵道運輸發達。鐵路由南滿洲鐵道株式會社經營，最主要的幹線為南滿鐵路。1935年，東清鐵路由滿洲國向蘇聯收購，再由滿鐵繼續鋪設鐵路。滿洲國以1.6億日元的價格收購長春至哈爾濱以及滿洲里至綏芬河的鐵路。其他重要鐵路還有丹東至奉天的安奉線、新京至圖們的京圖線、四平至齊齊哈爾的平齊線等。到1939年，路線全長已超過一萬公里，1945年達到11479公里，成為當時世界鐵道運輸最發達的國家之一。相較之下，1949年時全中國鐵路總里程僅22000公里。在大連與哈爾濱之間運營的「超特急」列車「亞細亞號」為當時滿鐵的象徵。1943年滿洲國公路總里程近6萬公里，而1949年，中國公路總里程僅有8.09萬公里。航運部分，主要港口有大連港和營口港；內河水運主要集中在松花江地區。\n",
            "營運滿洲國境內的鐵路的是什麼公司？\n",
            "南滿洲鐵道株式會社[SEP]\n",
            "最大長度: 499\n",
            "有符合限制長度的總數 17130\n",
            "6986 6986 6986\n",
            "從19世紀末至20世紀初湧入多倫多的移民主要是德國人、義大利人和來自東歐的猶太人。華人、俄羅斯人、波蘭人和其他東歐國家的移民後來相繼抵達。縱使人口增長迅速，直至1920年代，多倫多的人口和經濟影響力還是不及歷史較悠久的蒙特婁，但多倫多證券交易所卻於1934年成為全國最大的交易所。二戰後，來自東歐的難民，以及華人、義大利人和葡萄牙人陸續抵達。加拿大政府於1960年代末取消含有種族歧視成份的移民法後，更多移民從世界各個角落抵達多倫多。多倫多的人口於1951年越過一百萬大關，更於1971年增至二百萬人。而很多全國和跨國企業於1980年代有見於魁北克政局不穩，紛紛把總部從蒙特婁遷至多倫多和加拿大西部其他城市，也造就多倫多於1980年代取代蒙特婁成為全國最大城市和金融中心。\n",
            "什麼時間之前加拿大最大的城市和金融中心並非多倫多？\n",
            "1980年代[SEP]\n",
            "最大長度: 504\n",
            "有符合限制長度的總數 4308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJE0E8pEhs0E",
        "colab_type": "text"
      },
      "source": [
        "### **開始訓練**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GDqVz3kT9c7",
        "colab_type": "code",
        "outputId": "e62a0a65-4a8f-465f-888b-5410f9b804a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"時間 :\", time.asctime( time.localtime(time.time())) )\n",
        "#Step5:建構training loop、開始訓練 Start=========================================\n",
        "for epoch in range(20):\n",
        "    \n",
        "    All_train_correct = 0.0\n",
        "    AllTrainLoss = 0.0\n",
        "    count = 0\n",
        "    model.zero_grad() #梯度歸零\n",
        "    for batch_dict in train_dataloader:\n",
        "\n",
        "        # 訓練模式\n",
        "        model.train()\n",
        "        \n",
        "        batch_dict = tuple(t.to(device) for t in batch_dict)\n",
        "        #print(batch_dict[0].shape) #torch.Size([4, 512])\n",
        "        #print(batch_dict[1].shape) #torch.Size([4, 512])\n",
        "        #print(batch_dict[2].shape) #torch.Size([4, 512])\n",
        "        #print(batch_dict[3].shape) #torch.Size([4, 512])\n",
        "        #順序對應產⽣Dataset那裡的dataset = TensorDataset(all_token_embeddings, all_segement_embeddings, all_attention_mask, all_masked_labels)\n",
        "        outputs = model(\n",
        "            input_ids = batch_dict[0],\n",
        "            token_type_ids = batch_dict[1],\n",
        "            attention_mask = batch_dict[2],\n",
        "            masked_lm_labels = batch_dict[3]\n",
        "            )\n",
        "        loss, logits = outputs[:2]   \n",
        "\n",
        "        AllTrainLoss += loss.item()\n",
        "        count += 1\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "    Average_train_loss = round(AllTrainLoss/count, 3)\n",
        "\n",
        "    # 測試模式\n",
        "    model.eval()\n",
        "    All_test_correct = 0.0\n",
        "    AllTestLoss = 0.0\n",
        "    count = 0\n",
        "    for batch_dict in test_dataloader:\n",
        "        batch_dict = tuple(t.to(device) for t in batch_dict)  #每⼀個Batch把資料從CPU RAM 推到 GPU RAM\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids = batch_dict[0],\n",
        "            token_type_ids = batch_dict[1],\n",
        "            attention_mask = batch_dict[2],\n",
        "            masked_lm_labels = batch_dict[3]\n",
        "            )\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        AllTestLoss += loss.item()\n",
        "\n",
        "        count += 1\n",
        "        \n",
        "    Average_test_loss = round(AllTestLoss/count, 3)\n",
        "\n",
        "    print('第' + str(epoch+1) + '次' + '訓練模式，loss為:' + str(Average_train_loss) + '，測試模式，loss為:' + str(Average_test_loss) )\n",
        "    print (\"時間 :\", time.asctime( time.localtime(time.time())) )\n",
        "    print('---'*20)\n",
        "\n",
        "#Step5:建構training loop、開始訓練 END==========================================="
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "時間 : Mon Jun 15 02:30:27 2020\n",
            "第1次訓練模式，loss為:6.738，測試模式，loss為:6.073\n",
            "時間 : Mon Jun 15 02:54:10 2020\n",
            "------------------------------------------------------------\n",
            "第2次訓練模式，loss為:5.913，測試模式，loss為:5.937\n",
            "時間 : Mon Jun 15 03:17:49 2020\n",
            "------------------------------------------------------------\n",
            "第3次訓練模式，loss為:5.746，測試模式，loss為:5.794\n",
            "時間 : Mon Jun 15 03:41:30 2020\n",
            "------------------------------------------------------------\n",
            "第4次訓練模式，loss為:5.6，測試模式，loss為:5.667\n",
            "時間 : Mon Jun 15 04:05:24 2020\n",
            "------------------------------------------------------------\n",
            "第5次訓練模式，loss為:5.451，測試模式，loss為:5.612\n",
            "時間 : Mon Jun 15 04:29:12 2020\n",
            "------------------------------------------------------------\n",
            "第6次訓練模式，loss為:5.304，測試模式，loss為:5.514\n",
            "時間 : Mon Jun 15 04:52:50 2020\n",
            "------------------------------------------------------------\n",
            "第7次訓練模式，loss為:5.152，測試模式，loss為:5.432\n",
            "時間 : Mon Jun 15 05:16:43 2020\n",
            "------------------------------------------------------------\n",
            "第8次訓練模式，loss為:4.992，測試模式，loss為:5.409\n",
            "時間 : Mon Jun 15 05:40:34 2020\n",
            "------------------------------------------------------------\n",
            "第9次訓練模式，loss為:4.821，測試模式，loss為:5.309\n",
            "時間 : Mon Jun 15 06:04:21 2020\n",
            "------------------------------------------------------------\n",
            "第10次訓練模式，loss為:4.635，測試模式，loss為:5.235\n",
            "時間 : Mon Jun 15 06:28:09 2020\n",
            "------------------------------------------------------------\n",
            "第11次訓練模式，loss為:4.419，測試模式，loss為:5.106\n",
            "時間 : Mon Jun 15 06:51:44 2020\n",
            "------------------------------------------------------------\n",
            "第12次訓練模式，loss為:4.138，測試模式，loss為:4.811\n",
            "時間 : Mon Jun 15 07:15:29 2020\n",
            "------------------------------------------------------------\n",
            "第13次訓練模式，loss為:3.759，測試模式，loss為:4.422\n",
            "時間 : Mon Jun 15 07:39:19 2020\n",
            "------------------------------------------------------------\n",
            "第14次訓練模式，loss為:3.347，測試模式，loss為:4.119\n",
            "時間 : Mon Jun 15 08:03:06 2020\n",
            "------------------------------------------------------------\n",
            "第15次訓練模式，loss為:2.958，測試模式，loss為:3.878\n",
            "時間 : Mon Jun 15 08:26:54 2020\n",
            "------------------------------------------------------------\n",
            "第16次訓練模式，loss為:2.613，測試模式，loss為:3.727\n",
            "時間 : Mon Jun 15 08:50:47 2020\n",
            "------------------------------------------------------------\n",
            "第17次訓練模式，loss為:2.322，測試模式，loss為:3.561\n",
            "時間 : Mon Jun 15 09:14:39 2020\n",
            "------------------------------------------------------------\n",
            "第18次訓練模式，loss為:2.081，測試模式，loss為:3.532\n",
            "時間 : Mon Jun 15 09:38:30 2020\n",
            "------------------------------------------------------------\n",
            "第19次訓練模式，loss為:1.881，測試模式，loss為:3.537\n",
            "時間 : Mon Jun 15 10:02:25 2020\n",
            "------------------------------------------------------------\n",
            "第20次訓練模式，loss為:1.713，測試模式，loss為:3.489\n",
            "時間 : Mon Jun 15 10:26:16 2020\n",
            "------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "steBZuo9h28-",
        "colab_type": "text"
      },
      "source": [
        "### **儲存模型**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzqSOYDDT_hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#創⼀個資料夾存模型\n",
        "if not os.path.isdir(trained_Model_Path):\n",
        "    os.mkdir(trained_Model_Path)\n",
        "#儲存\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(trained_Model_Path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKhL6zJWBan",
        "colab_type": "code",
        "outputId": "ea9e6942-8a8d-4924-d73c-d3624e23e24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('train end')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train end\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}