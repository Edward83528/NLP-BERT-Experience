{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"i5doOG9iIsYg","colab_type":"text"},"source":["#**Answer Generation 任務**\n","資料集引用台達閱讀理解資料集\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sXKj-j9ZfzFu","colab_type":"text"},"source":["### pytorch環境設定(https://pytorch.org)\n","選擇適合的版本並複製指令貼到本專案"]},{"cell_type":"code","metadata":{"id":"7mUPOCc9f1fT","colab_type":"code","outputId":"7cea48be-05f8-4c25-9998-832d28eb1726","executionInfo":{"status":"ok","timestamp":1592114419394,"user_tz":-480,"elapsed":5431,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["!pip install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6CQquGs0f2-E","colab_type":"code","outputId":"5c421875-3927-4493-b54b-a16e43811d9f","executionInfo":{"status":"ok","timestamp":1592114419395,"user_tz":-480,"elapsed":5419,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["#確定有安裝成功\n","import torch\n","print(torch.__version__)\n","torch.cuda.is_available()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.5.0+cu101\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"KTf31QYifint","colab_type":"text"},"source":["### **取得 google drive 存取權限**"]},{"cell_type":"code","metadata":{"id":"8CqfyK8SIo2o","colab_type":"code","outputId":"b2aa420b-61de-43bf-ac82-e3dfd67b83ee","executionInfo":{"status":"ok","timestamp":1592114419792,"user_tz":-480,"elapsed":5805,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"72Pg2ymcGyg2","colab_type":"code","outputId":"5833a838-5759-4677-f6a4-b4f083c44fd0","executionInfo":{"status":"ok","timestamp":1592114419793,"user_tz":-480,"elapsed":5797,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive/Colab Notebooks/Bert/AnswerGeneration/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Bert/AnswerGeneration\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KdtNtU4DG02O","colab_type":"code","outputId":"a5eacaa7-056c-4d97-9e8a-6d2980603e7a","executionInfo":{"status":"ok","timestamp":1592114422370,"user_tz":-480,"elapsed":8365,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["albert\t       test.json      trained_model2  train.json\n","predict.ipynb  trained_model  train.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t_RZSbmA5iyI","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33rIRZCIj2yW","colab_type":"text"},"source":["### **下載dataset**"]},{"cell_type":"code","metadata":{"id":"_zDOYOkyj67s","colab_type":"code","colab":{}},"source":["import os\n","def fileExists(filepath):\n","  return  os.path.exists(filepath)\n","#我把資料集放到我的github\n","if fileExists('train.json')!=True:\n","  !wget -O train.json https://raw.githubusercontent.com/harry83528/NLP-BERT-Experience/master/AnswerGeneration/DataSet/DRCD_training.json\n","if fileExists('test.json')!=True:\n","  !wget -O test.json https://raw.githubusercontent.com/harry83528/NLP-BERT-Experience/master/AnswerGeneration/DataSet/DRCD_test.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ps1T6kk6j9s9","colab_type":"code","outputId":"e5278232-5549-4573-bdf0-f3bcf56b29e6","executionInfo":{"status":"ok","timestamp":1592114425594,"user_tz":-480,"elapsed":11567,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["albert\t       test.json      trained_model2  train.json\n","predict.ipynb  trained_model  train.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I5GCPnyJW94G","colab_type":"text"},"source":["##解析json"]},{"cell_type":"code","metadata":{"id":"8v4klQQiZwaL","colab_type":"code","outputId":"a533fea2-5216-4e80-f45c-e4684773475a","executionInfo":{"status":"ok","timestamp":1592114430224,"user_tz":-480,"elapsed":16189,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!pip install ijson"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ijson in /usr/local/lib/python3.6/dist-packages (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LqaPMTwBXeLN","colab_type":"code","colab":{}},"source":["import ijson\n","# 讀取數據\n","def LoadData(filepath):\n","    context = []\n","    question = []\n","    answersText = []\n","    count=0\n","    contextId=''\n","    with open(filepath) as file:\n","        parser = ijson.parse(file)\n","        contextId=''\n","        context2=''\n","        question2=''\n","        for prefix, event, value in parser:\n","            # print('prefix',prefix)\n","            # print('event',event)\n","            # print('value',value)\n","            if prefix==\"data.item.paragraphs.item.id\":\n","              #print('contextId',value)          \n","              contextId=value\n","            if prefix==\"data.item.paragraphs.item.context\":\n","              #print('context',value)\n","              context2=value\n","            if prefix==\"data.item.paragraphs.item.qas.item.question\":\n","              question2=value\n","              #print('question',value)\n","              #print('contextId',contextId)              \n","            if prefix==\"data.item.paragraphs.item.qas.item.answers.item.text\":\n","              #print('context2',context2)\n","              #print('answersText',value)\n","              #print('contextId',contextId)\n","              context.append(context2)\n","              question.append(question2)\n","              answersText.append(value+\"[SEP]\")\n","            #print(\"---\"*50)\n","            # count=count+1\n","            # if count==51:\n","            #   break;\n","    return context,question,answersText"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qQKy9Cntu0AA","colab_type":"text"},"source":["##下載Albert並放在albert目錄"]},{"cell_type":"code","metadata":{"id":"s4dSWkZOu2rA","colab_type":"code","outputId":"432b2e18-b0a8-40aa-d233-03250482c0e2","executionInfo":{"status":"ok","timestamp":1592114433246,"user_tz":-480,"elapsed":19196,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!git clone https://github.com/harry83528/albert-zh-for-pytorch-transformers.git albert"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fatal: destination path 'albert' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eMHSR-23g14r","colab_type":"text"},"source":["### **安裝所需的函式庫-HuggingFace 團隊將 GitHub 專案大翻新並更名成 transformers(Install the Hugging Face Library)**\n","https://github.com/huggingface/transformers"]},{"cell_type":"code","metadata":{"id":"noekyOhAg3qt","colab_type":"code","outputId":"3bdc2409-a87c-445e-e8a5-f2d92eeff39d","executionInfo":{"status":"ok","timestamp":1592114437559,"user_tz":-480,"elapsed":23500,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OKjsatBug5tU","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmTaWgVBvGx4","colab_type":"code","colab":{}},"source":["def use_model(model_name, config_file_path, model_file_path, vocab_file_path, num_labels):\n","    # 選擇模型並加載設定\n","    if(model_name == 'bert'):\n","        from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n","        model_config, model_class, model_tokenizer = (BertConfig, BertForSequenceClassification, BertTokenizer)\n","        config = model_config.from_pretrained(config_file_path,num_labels = num_labels)\n","        model = model_class.from_pretrained(model_file_path, from_tf=bool('.ckpt' in 'bert-base-chinese'), config=config)\n","        tokenizer = model_tokenizer(vocab_file=vocab_file_path)\n","        return model, tokenizer\n","    elif(model_name == 'albert'):\n","        from albert.albert_zh import AlbertConfig, AlbertTokenizer, AlbertForMaskedLM\n","        model_config, model_class, model_tokenizer = (AlbertConfig, AlbertForMaskedLM, AlbertTokenizer)\n","        config = model_config.from_pretrained(config_file_path)\n","        #config = model_config.from_pretrained(config_file_path,num_labels = num_labels) # AlbertForMaskedLM 不需分類,不用設num_labels\n","        model = model_class.from_pretrained(model_file_path, config=config)\n","        tokenizer = model_tokenizer.from_pretrained(vocab_file_path)\n","        return model, tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIfZwFodk2K_","colab_type":"text"},"source":["### **指定想要用的模型是哪一種**\n","到以下網址查詢模型與函數資訊\n","https://huggingface.co/transformers"]},{"cell_type":"code","metadata":{"id":"kKfJFywXXTeH","colab_type":"code","colab":{}},"source":["#通常我會在這設一些通用模型參數\n","# PRETRAINED_MODEL_Name =\"bert-base-chinese\"  # 指定 想要用的預訓練模型\n","trained_Model_Path = \"trained_model3\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRc990adg8W-","colab_type":"text"},"source":["## **引入需要⽤到的函式庫**"]},{"cell_type":"code","metadata":{"id":"FFqvPMRjg7nL","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import torch\n","from transformers import BertConfig, BertTokenizer, BertForMaskedLM, AdamW # 此任務不用\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F # 激勵函數\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFROMAiKl8ui","colab_type":"text"},"source":["### **載入資料並將資料轉換成輸入格式(Token Embeddings、Segment Embeddings、Position Embeddings)**"]},{"cell_type":"code","metadata":{"id":"6TyhZ9rNXgEZ","colab_type":"code","colab":{}},"source":["def to_feature(tokenizer, context, question, answer, token_embeddings, segement_embeddings, attention_mask, masked_labels, max_seq_len):\n","    segement_id = []\n","    attention_id = []\n","    masked_id = []\n","\n","    context_string = \"[CLS]\" + context + \"[SEP]\"\n","    context_tokenize = tokenizer.tokenize(context_string)\n","    for i in range(len(context_tokenize)):\n","        segement_id.append(0) # 第0組\n","        attention_id.append(1) # attention都是1告訴bert是有意義的文字\n","        masked_id.append(-1)  # 不是被mask的字都是-1\n","\n","    question_string = question + \"[SEP]\"\n","    question_tokenize = tokenizer.tokenize(question_string)\n","    for i in range(len(question_tokenize)):\n","        segement_id.append(1) # 第1組\n","        attention_id.append(1) # attention都是1告訴bert是有意義的文字\n","        masked_id.append(-1)  # 不是被mask的字都是-1\n","\n","    answer_tokenize = tokenizer.tokenize(answer)\n","    #print('answer_tokenize:',answer_tokenize); #['西', '元', '1994', '年', '5', '月', '28', '日','<SEP>']\n","    for index, answer in enumerate(answer_tokenize):\n","        input_string = context_tokenize + question_tokenize\n","        token_id = []\n","        input_segement_id = segement_id + []\n","        input_attention_id = attention_id + []\n","        input_masked_id = masked_id + []\n","        \n","        for i in range(index):\n","            input_string.append(answer_tokenize[i])\n","            input_segement_id.append(2) # 第2組\n","            input_attention_id.append(1) # attention都是1告訴bert是有意義的文字\n","            input_masked_id.append(-1)  # 不是被mask的字都是-1\n","\n","        input_string.append('[MASK]')\n","        input_segement_id.append(2) # 第2組\n","        input_attention_id.append(1) # attention都是1告訴bert是有意義的文字\n","        input_masked_id.append(tokenizer.convert_tokens_to_ids(answer))  # 被mask的掉字給該單詞tokens id\n","\n","        token_id = tokenizer.convert_tokens_to_ids(input_string)\n","        # print('input_string',input_string)\n","        # print('token_id',token_id)\n","        # print('input_segement_id',input_segement_id)\n","        # print('input_attention_id',input_attention_id)\n","        # print('input_masked_id',input_masked_id)\n","        # print('---'*20)\n","\n","        token_embeddings.append(token_id)\n","        segement_embeddings.append(input_segement_id)\n","        attention_mask.append(input_attention_id)\n","        masked_labels.append(input_masked_id)\n","\n","        if len(token_id) > max_seq_len:\n","            max_seq_len = len(token_id)\n","\n","    return max_seq_len\n","def convert_data_to_feature(filepath,tokenizer):\n","\n","    context,question,answersText = LoadData(filepath)\n","\n","    print(len(context),len(question),len(answersText))\n","    print(context[1000])\n","    print(question[1000])\n","    print(answersText[1000])\n","\n","    token_embeddings = []\n","    segement_embeddings = []\n","    attention_mask = []\n","    masked_labels = []\n","\n","    max_seq_len = 0\n","    conform_count = 0\n","    for i in range(0,len(context)-1):\n","        context_word_piece_list=tokenizer.tokenize(context[i])\n","        question_word_piece_list=tokenizer.tokenize(question[i])\n","        answersText_word_piece_list=tokenizer.tokenize(answersText[i])\n","        if len(context_word_piece_list) <= 450 and len(question_word_piece_list) <= 42 and len(answersText_word_piece_list) <= 16:\n","            max_seq_len = to_feature(tokenizer, context[i], question[i], answersText[i],token_embeddings,segement_embeddings,attention_mask,masked_labels,max_seq_len)\n","            conform_count =conform_count+1\n","\n","    print(\"最大長度:\",max_seq_len)\n","    print(\"有符合限制長度的總數\",conform_count)\n","\n","    assert max_seq_len <= 512 # 小於BERT-base長度限制\n","    max_seq_len = 512\n","\n","    # 不足長度都補齊長度,都補0\n","    for item in token_embeddings:\n","        while len(item)<max_seq_len:\n","            item.append(0)\n","\n","    for item in segement_embeddings:\n","        while len(item)<max_seq_len:\n","            item.append(0)\n","\n","    for item in attention_mask:\n","        while len(item)<max_seq_len:\n","            item.append(0)\n","\n","    for item in masked_labels:\n","        while len(item)<max_seq_len:\n","            item.append(-1)\n","    \n","    assert len(token_embeddings) == len(segement_embeddings) and len(token_embeddings) == len(attention_mask) and len(token_embeddings) == len(masked_labels)\n","\n","    #為了使⽤⽅便，把三個input embedding及 label 包成 object======================\n","    data_features = {'token_embeddings':token_embeddings,\n","                    'segement_embeddings':segement_embeddings,\n","                    'attention_mask':attention_mask,\n","                    'masked_labels':masked_labels}\n","\n","    return data_features\n","       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJa9hDtHha7d","colab_type":"text"},"source":["### **輸入格式轉Dataset**"]},{"cell_type":"code","metadata":{"id":"mmfCA--5giIM","colab_type":"code","colab":{}},"source":["def makeDataset(data_feature):\n","    token_embeddings = data_feature['token_embeddings']\n","    segement_embeddings = data_feature['segement_embeddings']\n","    attention_mask = data_feature['attention_mask']\n","    masked_labels = data_feature['masked_labels']\n","\n","    all_token_embeddings = torch.tensor([token_id for token_id in token_embeddings], dtype=torch.long)\n","    all_segement_embeddings = torch.tensor([segement_id for segement_id in segement_embeddings], dtype=torch.long)\n","    all_attention_mask = torch.tensor([attention_id for attention_id in attention_mask], dtype=torch.long)\n","    all_masked_labels = torch.tensor([masked_lm_id for masked_lm_id in masked_labels], dtype=torch.long)\n","\n","    return TensorDataset(all_token_embeddings, all_segement_embeddings, all_attention_mask, all_masked_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GD-OABDMhe8E","colab_type":"text"},"source":["## **確定 Embedding 是否正確**"]},{"cell_type":"markdown","metadata":{"id":"bJvLZ1tH2rgf","colab_type":"text"},"source":["## **分割數據**\n","把資料切成訓練集跟測試集\n","對於 Supervised Learning 來說，我們使⽤標註資料（Labeled Data）來做訓練\n","因此我們需要訓練資料（Training Data）\n","但是我們蒐集到的資料卻不能全部拿來做訓練\n","因為我們必須要保留⼀些當作測試資料（Testing Data）來評估模型表現\n","這些資料必須跟訓練資料是完全不同的，否則就有作弊的嫌疑\n","所以我們會把我們的資料切成訓練集與測試集，通常會保留比較多當作訓練資料\n","（60%~80%），其餘做測試資料"]},{"cell_type":"code","metadata":{"id":"ST5ZHdBt2BA9","colab_type":"code","colab":{}},"source":["def split_dataset(full_dataset, split_rate=0.8):  \n","    train_size = int(split_rate * len(full_dataset))\n","    test_size = len(full_dataset) - train_size\n","    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n","    return train_dataset,test_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qXGgr6iUhiRd","colab_type":"text"},"source":["### **Fine-Tuning**\n"]},{"cell_type":"code","metadata":{"id":"dN6qIwfDhlVV","colab_type":"code","colab":{}},"source":["# 計算正確值 (此次任務為生成不需要)\n","# def compute_accuracy(y_pred, y_target):\n","#     _, y_pred_indices = y_pred.max(dim=1)\n","#     n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","#     return n_correct / len(y_pred_indices) * 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTo7tDYqT7a8","colab_type":"code","outputId":"f6ed184c-9047-441d-c233-37517e994314","executionInfo":{"status":"ok","timestamp":1592114615684,"user_tz":-480,"elapsed":201569,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["# RuntimeError: Error(s) in loading state_dict for AlbertForMaskedLM:\n","# \tsize mismatch for bert.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([2, 312]) from checkpoint, the shape in current model is torch.Size([3, 312])\n","# if len(error_msgs) > 0:\n","#     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n","#                         model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n","#設定模型參數 Start =============================================================\n","#Step1:初始化加載模型 Start======================================================\n","model_setting = {\n","    \"model_name\":\"albert\", \n","    \"config_file_path\":\"albert/albert_tiny/config.json\", \n","    \"model_file_path\":\"albert/albert_tiny/pytorch_model.bin\", \n","    \"vocab_file_path\":\"albert/albert_tiny/vocab.txt\",\n","    \"num_labels\": -1 # 分幾類(AlbertForMaskedLM 不需分類,不用設num_labels,在這隨便亂設)\n","}\n","#初始化加載模型\n","model, tokenizer = use_model(**model_setting)\n","# config = BertConfig.from_pretrained(PRETRAINED_MODEL_Name)\n","# model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_Name, from_tf=bool('.ckpt' in PRETRAINED_MODEL_Name), config=config)\n","#Step1:初始化加載模型 END========================================================\n","\n","#Step2:指定硬體裝置 Start========================================================\n","# setting device\n","#你電腦的 GPU 能否被 PyTorch 調用,如果不行就使用CPU \n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"using device\",device)\n","model.to(device)\n","#Step2:指定硬體裝置 END==========================================================\n","\n","#Step3:將訓練資料讀入並且組建BERT輸入格式 Start===================================\n","train_data_feature = convert_data_to_feature('train.json',tokenizer)\n","test_data_feature = convert_data_to_feature('test.json',tokenizer)\n"," #Step3:將訓練資料讀入並且組建BERT輸入格式 END====================================\n","\n","#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader Srart======\n","# full_dataset = makeDataset(train_data_feature)\n","# train_dataset, test_dataset = split_dataset(full_dataset, 0.8)\n","train_dataset = makeDataset(train_data_feature)\n","test_dataset = makeDataset(test_data_feature)\n","\n","train_dataloader = DataLoader(train_dataset ,batch_size=2 ,shuffle=True)\n","test_dataloader = DataLoader(test_dataset ,batch_size=1 ,shuffle=True)\n","#Step4:將組建好的輸入格式轉換成tensor格式，並且建立dataset與dataloader END========\n","\n","#準備優化器 Start===============================================================\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","Learning_rate = 5e-6       # 學習率\n","optimizer = AdamW(optimizer_grouped_parameters, lr=Learning_rate, eps=1e-8)\n","#準備優化器 End=================================================================\n","\n","\n","#設定模型參數 End==============================================================="],"execution_count":0,"outputs":[{"output_type":"stream","text":["using device cuda\n","26936 26936 26936\n","滿洲國境內鐵路線稠密，鐵道運輸發達。鐵路由南滿洲鐵道株式會社經營，最主要的幹線為南滿鐵路。1935年，東清鐵路由滿洲國向蘇聯收購，再由滿鐵繼續鋪設鐵路。滿洲國以1.6億日元的價格收購長春至哈爾濱以及滿洲里至綏芬河的鐵路。其他重要鐵路還有丹東至奉天的安奉線、新京至圖們的京圖線、四平至齊齊哈爾的平齊線等。到1939年，路線全長已超過一萬公里，1945年達到11479公里，成為當時世界鐵道運輸最發達的國家之一。相較之下，1949年時全中國鐵路總里程僅22000公里。在大連與哈爾濱之間運營的「超特急」列車「亞細亞號」為當時滿鐵的象徵。1943年滿洲國公路總里程近6萬公里，而1949年，中國公路總里程僅有8.09萬公里。航運部分，主要港口有大連港和營口港；內河水運主要集中在松花江地區。\n","營運滿洲國境內的鐵路的是什麼公司？\n","南滿洲鐵道株式會社[SEP]\n","最大長度: 499\n","有符合限制長度的總數 17130\n","6986 6986 6986\n","從19世紀末至20世紀初湧入多倫多的移民主要是德國人、義大利人和來自東歐的猶太人。華人、俄羅斯人、波蘭人和其他東歐國家的移民後來相繼抵達。縱使人口增長迅速，直至1920年代，多倫多的人口和經濟影響力還是不及歷史較悠久的蒙特婁，但多倫多證券交易所卻於1934年成為全國最大的交易所。二戰後，來自東歐的難民，以及華人、義大利人和葡萄牙人陸續抵達。加拿大政府於1960年代末取消含有種族歧視成份的移民法後，更多移民從世界各個角落抵達多倫多。多倫多的人口於1951年越過一百萬大關，更於1971年增至二百萬人。而很多全國和跨國企業於1980年代有見於魁北克政局不穩，紛紛把總部從蒙特婁遷至多倫多和加拿大西部其他城市，也造就多倫多於1980年代取代蒙特婁成為全國最大城市和金融中心。\n","什麼時間之前加拿大最大的城市和金融中心並非多倫多？\n","1980年代[SEP]\n","最大長度: 504\n","有符合限制長度的總數 4308\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xJE0E8pEhs0E","colab_type":"text"},"source":["### **開始訓練**"]},{"cell_type":"code","metadata":{"id":"6GDqVz3kT9c7","colab_type":"code","outputId":"9d335476-380f-4767-bf1b-dd33b5370205","executionInfo":{"status":"ok","timestamp":1592120626133,"user_tz":-480,"elapsed":6212011,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["#Step5:建構training loop、開始訓練 Start=========================================\n","for epoch in range(5):\n","    \n","    All_train_correct = 0.0\n","    AllTrainLoss = 0.0\n","    count = 0\n","    model.zero_grad() #梯度歸零\n","    for batch_dict in train_dataloader:\n","\n","        # 訓練模式\n","        model.train()\n","        \n","        batch_dict = tuple(t.to(device) for t in batch_dict)\n","        #print(batch_dict[0].shape) #torch.Size([4, 512])\n","        #print(batch_dict[1].shape) #torch.Size([4, 512])\n","        #print(batch_dict[2].shape) #torch.Size([4, 512])\n","        #print(batch_dict[3].shape) #torch.Size([4, 512])\n","        #順序對應產⽣Dataset那裡的dataset = TensorDataset(all_token_embeddings, all_segement_embeddings, all_attention_mask, all_masked_labels)\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],\n","            attention_mask = batch_dict[2],\n","            masked_lm_labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]   \n","\n","        AllTrainLoss += loss.item()\n","        count += 1\n","\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","            \n","    Average_train_loss = round(AllTrainLoss/count, 3)\n","\n","    # 測試模式\n","    model.eval()\n","    All_test_correct = 0.0\n","    AllTestLoss = 0.0\n","    count = 0\n","    for batch_dict in test_dataloader:\n","        batch_dict = tuple(t.to(device) for t in batch_dict)  #每⼀個Batch把資料從CPU RAM 推到 GPU RAM\n","\n","        outputs = model(\n","            input_ids = batch_dict[0],\n","            token_type_ids = batch_dict[1],\n","            attention_mask = batch_dict[2],\n","            masked_lm_labels = batch_dict[3]\n","            )\n","        loss, logits = outputs[:2]\n","\n","        AllTestLoss += loss.item()\n","\n","        count += 1\n","        \n","    Average_test_loss = round(AllTestLoss/count, 3)\n","\n","    print('第' + str(epoch+1) + '次' + '訓練模式，loss為:' + str(Average_train_loss) + '，測試模式，loss為:' + str(Average_test_loss) )\n","    print (\"時間 :\", time.asctime( time.localtime(time.time())) )\n","    print('---'*20)\n","\n","#Step5:建構training loop、開始訓練 END==========================================="],"execution_count":0,"outputs":[{"output_type":"stream","text":["第1次訓練模式，loss為:6.751，測試模式，loss為:6.081\n","時間 : Sun Jun 14 06:23:38 2020\n","------------------------------------------------------------\n","第2次訓練模式，loss為:5.92，測試模式，loss為:5.913\n","時間 : Sun Jun 14 06:43:38 2020\n","------------------------------------------------------------\n","第3次訓練模式，loss為:5.737，測試模式，loss為:5.771\n","時間 : Sun Jun 14 07:03:38 2020\n","------------------------------------------------------------\n","第4次訓練模式，loss為:5.507，測試模式，loss為:5.495\n","時間 : Sun Jun 14 07:23:47 2020\n","------------------------------------------------------------\n","第5次訓練模式，loss為:5.099，測試模式，loss為:4.999\n","時間 : Sun Jun 14 07:43:48 2020\n","------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"steBZuo9h28-","colab_type":"text"},"source":["### **儲存模型**"]},{"cell_type":"code","metadata":{"id":"CzqSOYDDT_hA","colab_type":"code","colab":{}},"source":["#創⼀個資料夾存模型\n","if not os.path.isdir(trained_Model_Path):\n","    os.mkdir(trained_Model_Path)\n","#儲存\n","model_to_save = model.module if hasattr(model, 'module') else model\n","model_to_save.save_pretrained(trained_Model_Path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCKhL6zJWBan","colab_type":"code","outputId":"a02a3bcb-51a1-40e8-ce92-2558bc01a190","executionInfo":{"status":"ok","timestamp":1592120626136,"user_tz":-480,"elapsed":6211997,"user":{"displayName":"張仲威","photoUrl":"","userId":"14727305247088321457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('train end')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train end\n"],"name":"stdout"}]}]}